---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Perceptron Learning Rule

x<- matriz de variables explicativas
y <- vector de variable a explicar
w <- número de parámetros, tantos como variables explicativas haya.
R <- criterio de penalización, se obtiene como el máximo de las posibles normas eucladianas de cada fila de la matriz x.
La normas eucladiana: (sqrt(p*p)) del vector depende la longitud de dicha vector. 

En la función DistanceFromPlane Z es el X de la función Classify Linear.

Se llama distancia from plane pero realmente es el valor de la función al multiplicar el valor de los parámetros por la x. Si el valor de la función es inferior a cero (le asignamos el valor -1) y si es valor superior a cero (le asignamos el valor 1).

El bucle compara y(i): valor real frente al valor estimado yc(i)

si en algun valor difiere, cambio el valor de los parámetros.
Como se cambia el valor de los parámetros (metiendole una proporcion que depende de la tasa de crecimiento), si el valor estimado (yc[i]) es -1 el valor de y[i], el parámetro te resta en la proporción del valor de cada elemento en la ecuación.


```{r}
# w, x, y are vectors

w = 0

# b parameter must also be included for the peceptron algorithm to deliver a valid separator.
# for incorrectly classified training samples b must be adjusted, too:

# while any training observation (x, y) is not classified correcty {
#   set w = w + learning_rate * yx
#   set b = b + learning_rate * yR^2
  # where R is some constant larger than the distance from the
  # origin to the furtherest training sample
# }
```

Función de Perceptron:

Para definir esta función hay que definir previamente tres funciones:

- DistanceFromPlane: te calcula el valor de la función, en un primer momento con el valor de los parametros inicializado
- EuclideanNorm: te calcula norma euclidea del vector de x. Es la raiz cuadrada de la suma de los Xi elevados al cuadrado.
- ClassifyLinear: Esta función realiza la predicción, clasifica a un individuo en 1 o -1 en función del valor de la función.

En el descenso del gradiente el criterio para modificar el valor de los parametros, es el gradiente o la derivada de la función según el parámetro.


```{r}
DistanceFromPlane = function(z, w, b) {
  sum(z * w) + b
}
ClassifyLinear = function(x, w, b) {
  distances = apply(x, 1, DistanceFromPlane, w, b)
  return(ifelse(distances < 0, -1, +1))
}

EuclideanNorm <- function(x) {
  return(sqrt(sum(x * x)))
}

PerceptronFunction <- function(x, y, learning.rate) {
  vector_iteraciones <- NULL
  contador <- 0
  vector_errores <- NULL
  w = vector(length = ncol(x)) # initialize w
  b = 0 # Initialize b
  iterations = 0 # count iterations
  R = max(apply(x, 1, EuclideanNorm))
  convergence = FALSE # to enter the while loop
  while (!convergence & iterations < 50000) { #si queremos parar ponemos un And con número de iteraciones y entonces pararía. 
    convergence = TRUE # hopes luck
    yc <- ClassifyLinear(x, w, b) #clasifica cada elemento de X.
    for (i in 1:nrow(x)) {
      if (y[i] != yc[i]) {
        convergence <- FALSE
        w <- w + learning.rate * y[i] * x[i,] #se trata de multiplicar el verdadero valor del parametro por la fila con todos los valores de la fila x correspondiente que es un vector. Se suma al vector de coeficientes y los modifica en ese sentido
        b <- b + learning.rate * y[i] * R^2
        iterations <- iterations + 1
        vector_iteraciones <- c(vector_iteraciones,iterations)
        y_pred <- ClassifyLinear(x, w, b)
        vector_errores <- c(vector_errores,sum(abs(y - y_pred)))
      }
    }
  }
s = EuclideanNorm(w) #se hace la norma euclidea del vector de los parametros para normalizar el valor de los mismos. 

df <- data.frame(vector_iteraciones,vector_errores)

return(list(w = w/s, b = b/s, steps = iterations, datos = df))
}
```


```{r}
PerceptronFunction2 <- function(x, y, learning.rate) {

  w <- vector(length = ncol(x)) # initialize w

  b <- 0 # Initialize b

  # initialize empty dataframe

  errorEvolutions <- data.frame(cycles = numeric(), error = numeric()) 

  # initialize cycles

  cycles <- 0

  iterations <- 0 # count iterations (changes)

  R = max(apply(x, 1, EuclideanNorm))

  convergence <- FALSE # to enter the while loop

  while (!convergence) {

    convergence <- TRUE # hopes luck

    yc <- ClassifyLinear(x, w, b)

    # update cycles

    cycles <- cycles + 1

    # update error for this cycle

    errorEvolutions <- rbind(errorEvolutions, c(cycles, sum(abs(y - yc))))

    for (i in 1:nrow(x)) {

      if (y[i] != yc[i]) {

        convergence <- FALSE

        w <- w + learning.rate * y[i] * x[i,]

        b <- b + learning.rate * y[i] * R^2

        iterations <- iterations + 1

      }

    }

  }

  s = EuclideanNorm(w)

  return(list(w = w/s, b = b/s, steps = iterations, errorEvolution = errorEvolutions))

}
```



```{r}
# very easy
# x2 = x1 + 1/2
x1 <- runif(50,-1,1)
x2 <- runif(50,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)

PlotData <- function(x, y) {
  plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
  abline(0.5,1)
  points(c(0,0), c(0,0), pch = 19)
  lines(c(0,-0.25), c(0,0.25), lty = 2)
  arrows(-0.3, 0.2, -0.4, 0.3)
  text(-0.45, 0.35, "w", cex = 2)
  text(-0.0, 0.15, "b", cex = 2)
}

PlotData(x, y)
```


Si queremos ir modificando la learning rate
```{r}
learning_rate <- 0.01
the_perceptron <- PerceptronFunction(x,y,learning_rate)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(sum(abs(y - predicted_y)))

```

1. Saber el numero de iteraciones

```{r}
the_perceptron$steps
```


2. Representación grafica

```{r}
df <- the_perceptron$datos
plot(df$vector_iteraciones,df$vector_errores)
```



3. Dataframe con muchos datos, buscando algo que no converga (sin tener que utilizar funciones muy complejas).



```{r}
# very easy
# x2 = x1 + 1/2
x1b <- runif(500,-1,1)
x2b <- runif(500,-1,1)
xb <- cbind(x1b,x2b)
yb <- ifelse(x2b > 0.5 + x1b, +1, -1)
PlotData <- function(x, y) {
  plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
  abline(0.5,1)
  points(c(0,0), c(0,0), pch = 19)
  lines(c(0,-0.25), c(0,0.25), lty = 2)
  arrows(-0.3, 0.2, -0.4, 0.3)
  text(-0.45, 0.35, "w", cex = 2)
  text(-0.0, 0.15, "b", cex = 2)
}
PlotData(xb, yb)

```




4. Modificar el algoritmo para que sino converga pare: introducir un criterio de parada en función del número de iteraciones (probarlo con un número muy alto de iteraciones).


Errores con la función de Perceptron que acumula errores cada vez que se cambia el valor de los parámetros (dentro del IF)

```{r}
learning_rate <- 0.1
the_perceptron <- PerceptronFunction(xb,yb,learning_rate)
predicted_y <- ClassifyLinear(xb, the_perceptron$w, the_perceptron$b)
```

Grafico

```{r}
df1 <- the_perceptron$datos
plot(df1$vector_iteraciones,df1$vector_errores)
```


Errores con la función de Perceptron que acumula errores cada vez que termina de recorrer todos las filas de lal vector y (dentro del while)

```{r}
learning_rate <- 0.1
the_perceptron2 <- PerceptronFunction2(xb,yb,learning_rate)
predicted_y <- ClassifyLinear(xb, the_perceptron$w, the_perceptron$b)
```



```{r}
the_perceptron2$steps
```


2. Representación grafica

```{r}
df2 <- the_perceptron2$errorEvolution
plot(df2[,1],df2[,2])
```






